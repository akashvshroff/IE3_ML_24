# AiSL

- A real-time American Sign Language translator that uses a Long Short Term Memory (LSTM) neural network to translate webcam footage into English. Videos are processed using MediaPipe and OpenCV, and the model is trained using PyTorch. 
- The project was completed as part of the IEEE technical program by Anant Poddar, Anoushka Sarup, Artebeth Yan, Emma Scally, Hannah Ma, Michael Lee under the guidance of Akash Vikram Shroff. 
- Training data for the model was largely collected form the WLASL dataset, along with the ASLLVD collection. In its entirety, there were apprximately 43,000 data points (i.e ASL words and phrases) for about 4000 of the most popular ASL words and phrases. The model converged to roughly 80% accuracy.

https://github.com/akashvshroff/IE3_ML_24/assets/63399889/33ccbd94-89ba-4da7-89b2-8aa1691898be

